#!/bin/bash

# Array of queries to search
#declare -a queries=('bunny' 'rabbit' 'cute bunny' 'bunny gif')

# Set up working directory
cd "$(dirname "${BASH_SOURCE[0]}")" && cd "$(git rev-parse --show-toplevel)" # pwd -> git repo's root
mkdir -p data/queries/
mkdir -p data/html/
mkdir bin/
mkdir obj/


HELP() {
echo "Usage: geturls.sh [OPTIONS] [QUERY] ... 
Retrieve a lists of URLs for each QUERY.

Mandatory arguments to long options are mandatory for short options too.
  -c, --count=NUM           Number of results for each query (max: 64)
  -f, --force-refresh       Overwrite cached results
  -h, --help                Show this help text
  -n, --no-html             Only compile URL list; do not generate html
  -t, --title=STRING        Title of the generated webpage
"
}

# Command line arguments
USE_CACHE=true
nitems=64
title=
generate_html=true
if ! options=$(getopt -o c:fhnt: -l count:,force-refresh,help,no-html,title: -- "$@"); then
	HELP
	exit 1;
fi
eval set -- $options

while [ $# -gt 0 ]; do
	case "$1" in
		-c | --count) 
			nitems="$2"
			shift
			if [[ ! $nitems =~ ^[0-9]+$ ]]; then # NaN
				echo "geturls: --count: invalid results count: $nitems" >&2
				exit 1
			fi
			if [[ $nitems -gt 64 ]]; then
				echo "geturls: --count: Maximum number of results is 64." >&2
				exit 1
			fi
			;;
		-f | --force-refresh) USE_CACHE=false;;
		-h | --help) HELP; exit 0;;
		-n | --no-html) generate_html=false;;
		-t | --title) title="$2"; shift;;
		(--) ;;
		(-*) echo "$0: unrecognized option $1" >&2; exit 1;;
		(*) queries[${#queries[@]}]="$1";;
	esac
	shift
done


if [ ${#queries[@]} -eq 0 ]; then
	echo "geturls: no queries. Exiting." >&2
	exit 1
fi
# Replace spaces with URL escape characters, trim whitespace
for ((i = 0; i < ${#queries[@]}; i++)); do
	queries["$i"]=$(echo "${queries[$i]}" | sed -e 's/^ //' -e 's/ $//' -e 's/ /%20/g')
done


# Returns Google Search API's response
# Arguments:
# $1: query
# $2: starting index (max 64 - $3)
# $3: number of results (max 8)
extract() {
		echo $(curl -s "http://ajax.googleapis.com/ajax/services/search/images?q=$1&v=1.0&start=$2&rsz=$3" | 
				sed 's/,/\n/g' | grep unescapedUrl | awk -F'"' '{print $4}')
}

# Get image URLs
for query in ${queries[@]}; do
	printf "Retrieving results for \'%s\'..." $query

	# Cache results
	if [[ "$USE_CACHE" = "true" && -f data/queries/"$query".txt ]]; then
		i=$(wc -l data/queries/"$query".txt 2>/dev/null | awk '{print $1}')
		cached_results="($i cached)"
	else
		rm -f data/queries/"$query".txt 2>/dev/null
		i=0
		cached_results=
	fi


	urls=
	pcnt=
	for ((; i < nitems-8; i+=8)); do
		# Loading percent
		pcnt=$(bc <<< "scale = 3; $i/$nitems*100" | awk -F. '{print $1}')
		printf "%.0f%%" $pcnt

		# Extract URLs
		urls+="$(extract $query $i 8) "

		# Erase current percentage
		if [[ $(($i+16)) -lt $nitems ]]; then
			[[ "$pcnt" -lt 10 ]] && printf "\b\b" || printf "\b\b\b"
		fi
	done

	# Cleanup iteration
	if [ "$i" -ne "$nitems" ]; then
		urls+=$(extract "$query" "$i" "$(($nitems-$i))" )
	fi

	# Write to file
	if [ -n "$urls" ]; then
		echo "$urls" | sed -e 's/\s*$//' -e 's/ /\n/g' >> data/queries/"$query".txt
	fi

	# 100%
	[[ "$pcnt" -lt 10 ]] && printf "\b\b" || printf "\b\b\b"
	printf "100%% %s\n" "$cached_results"
done


# Compile list
for((i = 0; i < ${#queries[@]}; i++)); do
	filenames[$i]=data/queries/"${queries[$i]}".txt
done
#cat ${filenames[@]} | sort | uniq > data/urls.txt
cat ${filenames[@]} > data/urls.txt

# Generate HTML file
if [ "$generate_html" = "true" ]; then
	echo "Generating html file..."
	[[ -z "$title" ]] && title=$(echo ${queries[@]} | sed 's/ / | /g' | sed 's/%20/ /g')
	make -s to-html
	bin/to-html --title "$title" --all < data/urls.txt > grid.html
	bin/to-html --title "$title" < data/urls.txt > rand.html
fi
