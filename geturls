#!/bin/bash

# Set up working directory
cd "$(dirname "${BASH_SOURCE[0]}")" && cd "$(git rev-parse --show-toplevel)" # pwd -> git repo's root
mkdir -p queries/ html/ bin/ obj/ 2>/dev/null


HELP() {
echo "Usage: geturls [OPTIONS] [QUERY] ... 
Retrieve a lists of URLs for each QUERY.

  -c, --count=NUM           Number of results for each query (max: 64)
  -f, --force-refresh       Overwrite cached results
  -h, --help                Show this help text
  -n, --no-html             Only compile URL list; do not generate html
  -t, --title=STRING        Title of the generated webpage
"
}

# Command line arguments
USE_CACHE=true
nitems=64
title=
generate_html=true
if ! options=$(getopt -o c:fhnt: -l count:,force-refresh,help,no-html,title: -- "$@"); then
	HELP
	exit 1;
fi
eval set -- $options

while [ $# -gt 0 ]; do
	case "$1" in
		-c | --count) 
			nitems="$2"
			shift
			if [[ ! $nitems =~ ^[0-9]+$ ]]; then # NaN
				echo "geturls: --count: invalid results count: $nitems" >&2
				exit 1
			fi
			if [[ $nitems -gt 64 ]]; then
				echo "geturls: --count: Maximum number of results is 64." >&2
				exit 1
			fi
			;;
		-f | --force-refresh) USE_CACHE=false;;
		-h | --help) HELP; exit 0;;
		-n | --no-html) generate_html=false;;
		-t | --title) title="$2"; shift;;
		(--) ;;
		(-*) echo "$0: unrecognized option $1" >&2; exit 1;;
		(*) queries[${#queries[@]}]="$1";;
	esac
	shift
done


if [ ${#queries[@]} -eq 0 ]; then
	echo "geturls: no queries. Exiting." >&2
	exit 1
fi
# Replace spaces with URL escape characters, trim whitespace
for ((i = 0; i < ${#queries[@]}; i++)); do
	queries["$i"]=$(echo "${queries[$i]}" | sed -e 's/^ //' -e 's/ $//' -e 's/ /%20/g')
done


# Returns Google Search API's response
# Arguments:
# $1: query
# $2: starting index (max 64 - $3)
# $3: number of results (max 8)
extract() {
		echo $(curl -s "http://ajax.googleapis.com/ajax/services/search/images?q=$1&v=1.0&start=$2&rsz=$3" | 
				sed 's/,/\n/g' | grep unescapedUrl | awk -F'"' '{print $4}')
}

# Get image URLs
for query in ${queries[@]}; do
	printf "Retrieving results for \'%s\'..." "$(echo "$query" | sed 's/%20/ /g')"

	# Cache results
	if [[ "$USE_CACHE" = "true" && -f queries/"$query".txt ]]; then
		i=$(wc -l queries/"$query".txt 2>/dev/null | awk '{print $1}')
		cached_results="($i cached)"
	else
		rm -f queries/"$query".txt 2>/dev/null
		i=0
		cached_results=
	fi


	urls=
	pcnt="unchanged"
	for ((; i < nitems-8; i+=8)); do
		# Loading percent
		pcnt=$(bc <<< "scale = 3; $i/$nitems*100" | awk -F. '{print $1}')
		printf "%.0f%%" $pcnt

		# Extract URLs
		urls+="$(extract $query $i 8) "

		# Erase current percentage
		if [[ $(($i+16)) -lt $nitems ]]; then
			[[ "$pcnt" -lt 10 ]] && printf "\b\b" || printf "\b\b\b"
		fi
	done

	# Cleanup iteration
	if [[ "$USE_CACHE" = false && "$i" -lt "$nitems" ]]; then
		urls+=$(extract "$query" "$i" "$(($nitems-$i))" )
	fi

	# Write to file
	if [ -n "$urls" ]; then
		echo "$urls" | sed -e 's/\s*$//' -e 's/ /\n/g' >> queries/"$query".txt
	fi

	# 100%
	if [[ "$pcnt" != "unchanged" ]]; then
		[[ "$pcnt" -lt 10 ]] && printf "\b\b" || printf "\b\b\b"
	fi
	printf "100%% %s\n" "$cached_results"
done


# Compile list
for((i = 0; i < ${#queries[@]}; i++)); do
	filenames[$i]=queries/"${queries[$i]}".txt
done
#cat ${filenames[@]} | sort | uniq > urls.txt
cat ${filenames[@]} > urls.txt

# Generate HTML file
if $generate_html; then
	# Default html title is "query1 | query2 | ..."
	[[ -z "$title" ]] && title=$(echo ${queries[@]} | sed 's/ / | /g' | sed 's/%20/ /g')

	make -s to-html clean-symlinks

	# Default html filename is "query1_query-with-spaces_query3_{rand,grid}.html
	ftitle=$(echo "$title" | sed -e 's/ | /_/g' -e 's/ /-/g')
	filepaths[0]=html/"$ftitle"_rand.html
	filepaths[1]=html/"$ftitle"_grid.html

	for filepath in ${filepaths[@]}; do
		if [[ "$USE_CACHE" = true  && -f "$filepath" ]]; then
			echo "$filepath cached"
		else
			echo "Generating $filepath..."

			# Set arguments
			tohtml_args="--title \"$title\""
			[[ "$filepath" = ${filepaths[1]} ]] && tohtml_args+=" --all"
			echo "$to_html_args" | xargs bin/to-html < urls.txt > $filepath
		fi
	done
		ln -s ${filepaths[0]} rand-prev.html >/dev/null
		ln -s ${filepaths[1]} grid-prev.html >/dev/null
else
	echo "Skipped generating html files"
fi

exit 0

